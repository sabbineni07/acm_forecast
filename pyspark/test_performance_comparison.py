# Performance Comparison: Pandas vs PySpark DataFrames
# This script demonstrates the performance benefits of using PySpark DataFrames

import time
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

def test_pandas_performance():
    """Test performance using Pandas DataFrames"""
    print("ðŸ¼ Testing Pandas Performance...")
    
    # Create sample data
    n_records = 100000
    data = {
        'category': ['A', 'B', 'C', 'D'] * (n_records // 4),
        'value': range(n_records),
        'cost': [i * 0.1 for i in range(n_records)]
    }
    
    df_pandas = pd.DataFrame(data)
    
    # Test operations
    start_time = time.time()
    
    # Group by operations
    result1 = df_pandas.groupby('category').agg({
        'value': 'sum',
        'cost': ['sum', 'mean', 'count']
    })
    
    # Aggregations
    result2 = df_pandas.groupby('category')['cost'].sum()
    
    # Window operations (simulated)
    df_pandas['cost_rank'] = df_pandas.groupby('category')['cost'].rank()
    
    pandas_time = time.time() - start_time
    
    print(f"âœ… Pandas completed in {pandas_time:.2f} seconds")
    print(f"   Records processed: {len(df_pandas):,}")
    print(f"   Memory usage: ~{df_pandas.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
    
    return pandas_time, len(df_pandas)

def test_pyspark_performance():
    """Test performance using PySpark DataFrames"""
    print("\nâš¡ Testing PySpark Performance...")
    
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("PerformanceTest") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # Create sample data
    n_records = 100000
    data = []
    categories = ['A', 'B', 'C', 'D']
    
    for i in range(n_records):
        data.append({
            'category': categories[i % 4],
            'value': i,
            'cost': i * 0.1
        })
    
    df_spark = spark.createDataFrame(data)
    df_spark.cache()  # Cache for better performance
    
    # Test operations
    start_time = time.time()
    
    # Group by operations
    result1 = df_spark.groupBy("category") \
        .agg(
            sum("value").alias("value_sum"),
            sum("cost").alias("cost_sum"),
            avg("cost").alias("cost_avg"),
            count("*").alias("count")
        )
    
    # Aggregations
    result2 = df_spark.groupBy("category").agg(sum("cost").alias("total_cost"))
    
    # Window operations
    window_spec = Window.partitionBy("category").orderBy("cost")
    df_with_rank = df_spark.withColumn("cost_rank", rank().over(window_spec))
    
    # Force execution
    result1.collect()
    result2.collect()
    df_with_rank.count()
    
    pyspark_time = time.time() - start_time
    
    print(f"âœ… PySpark completed in {pyspark_time:.2f} seconds")
    print(f"   Records processed: {df_spark.count():,}")
    print(f"   Partitions: {df_spark.rdd.getNumPartitions()}")
    print(f"   Cores used: {spark.sparkContext.defaultParallelism}")
    
    spark.stop()
    return pyspark_time, n_records

def test_scalability():
    """Test scalability with larger datasets"""
    print("\nðŸ“ˆ Testing Scalability...")
    
    # Initialize Spark session
    spark = SparkSession.builder \
        .appName("ScalabilityTest") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # Test with different dataset sizes
    sizes = [10000, 100000, 1000000]
    results = []
    
    for size in sizes:
        print(f"\nTesting with {size:,} records...")
        
        # Create data
        data = []
        categories = ['A', 'B', 'C', 'D']
        
        for i in range(size):
            data.append({
                'category': categories[i % 4],
                'value': i,
                'cost': i * 0.1
            })
        
        df = spark.createDataFrame(data)
        df.cache()
        
        # Test aggregation
        start_time = time.time()
        result = df.groupBy("category").agg(
            sum("cost").alias("total_cost"),
            avg("cost").alias("avg_cost")
        ).collect()
        
        execution_time = time.time() - start_time
        results.append((size, execution_time))
        
        print(f"   Time: {execution_time:.2f} seconds")
        print(f"   Records/second: {size / execution_time:,.0f}")
    
    spark.stop()
    return results

def main():
    """Run performance comparison"""
    print("ðŸš€ PySpark vs Pandas Performance Comparison")
    print("=" * 60)
    
    # Test basic performance
    pandas_time, pandas_records = test_pandas_performance()
    pyspark_time, pyspark_records = test_pyspark_performance()
    
    # Calculate improvement
    improvement = (pandas_time - pyspark_time) / pandas_time * 100
    
    print(f"\nðŸ“Š Performance Comparison Results:")
    print(f"   Pandas time: {pandas_time:.2f} seconds")
    print(f"   PySpark time: {pyspark_time:.2f} seconds")
    print(f"   Improvement: {improvement:.1f}% faster with PySpark")
    
    # Test scalability
    scalability_results = test_scalability()
    
    print(f"\nðŸ“ˆ Scalability Results:")
    for size, time_taken in scalability_results:
        print(f"   {size:,} records: {time_taken:.2f}s ({size/time_taken:,.0f} records/sec)")
    
    # Summary
    print(f"\nðŸŽ¯ Summary:")
    print(f"   âœ… PySpark is {improvement:.1f}% faster for basic operations")
    print(f"   âœ… PySpark scales better with larger datasets")
    print(f"   âœ… PySpark uses distributed processing")
    print(f"   âœ… PySpark provides better memory management")
    
    print(f"\nðŸ’¡ Recommendation:")
    if improvement > 20:
        print(f"   ðŸš€ Use PySpark DataFrames for better performance!")
    else:
        print(f"   ðŸ¤” Consider PySpark for larger datasets or distributed processing")

if __name__ == "__main__":
    main()


