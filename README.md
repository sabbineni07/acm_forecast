# Azure Cost Management Forecasting Framework

A production-ready, plugin-based forecasting framework for Azure cost management using Prophet, ARIMA, and XGBoost models with PySpark-based data processing.

## ğŸ—ï¸ Project Structure

```
acm_forecast/
â”œâ”€â”€ acm_forecast/               # Main source code package
â”‚   â”œâ”€â”€ config/                 # YAML-based configuration system
â”‚   â”‚   â”œâ”€â”€ config.example.yaml # Example configuration template
â”‚   â”‚   â””â”€â”€ specifications.py   # Pydantic configuration classes
â”‚   â”œâ”€â”€ core/                   # Core framework components
â”‚   â”‚   â”œâ”€â”€ app_runner.py       # AppRunner - main orchestration class
â”‚   â”‚   â”œâ”€â”€ plugin_registry.py  # PluginFactory for dynamic plugin loading
â”‚   â”‚   â”œâ”€â”€ base_plugin.py      # Base plugin interface
â”‚   â”‚   â””â”€â”€ interfaces.py       # Plugin interface definitions
â”‚   â”œâ”€â”€ plugins/                # Pluggable components (plugin architecture)
â”‚   â”‚   â”œâ”€â”€ data_source/        # Data source plugins (acm)
â”‚   â”‚   â”œâ”€â”€ data_quality/       # Data quality plugins (default)
â”‚   â”‚   â”œâ”€â”€ data_preparation/   # Data preparation plugins (acm)
â”‚   â”‚   â”œâ”€â”€ feature_engineer/   # Feature engineering plugins (default)
â”‚   â”‚   â”œâ”€â”€ models/             # Model plugins (prophet, arima, xgboost)
â”‚   â”‚   â”œâ”€â”€ forecasters/        # Forecaster plugins (default)
â”‚   â”‚   â””â”€â”€ model_registry/     # Registry plugins (mlflow)
â”‚   â”œâ”€â”€ pipeline/               # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â”‚   â””â”€â”€ forecast_pipeline.py
â”‚   â”œâ”€â”€ evaluation/             # Model evaluation and metrics
â”‚   â”œâ”€â”€ monitoring/             # Performance monitoring and retraining
â”‚   â””â”€â”€ examples/               # Example scripts
â”‚       â”œâ”€â”€ run_end_to_end.py   # Complete end-to-end example
â”‚       â”œâ”€â”€ run_training.py     # Training pipeline example
â”‚       â””â”€â”€ run_forecast.py     # Forecast generation example
â”œâ”€â”€ tests/                      # Test suite (unit, integration, e2e)
â”œâ”€â”€ pyproject.toml              # Package configuration
â”œâ”€â”€ Makefile                    # Build and development commands
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Features

- **Plugin-Based Architecture**: Flexible, extensible plugin system for all components
- **Multiple Forecasting Models**: Prophet, ARIMA, and XGBoost
- **Production-Ready**: MLflow integration, monitoring, and automated retraining
- **PySpark-Based**: Scalable distributed processing for large datasets
- **YAML Configuration**: Simple, maintainable configuration management
- **AppRunner**: Simple high-level interface for running pipelines

## ğŸš€ Quick Start

### Installation

See [INSTALLATION.md](INSTALLATION.md) for detailed setup instructions.

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install package in development mode
pip install -e ".[dev]"
```

### Using Makefile

```bash
# Build wheel package
make build

# Run tests
make test

# Run specific test types
make test-unit          # Unit tests only
make test-integration   # Integration tests only
make test-e2e          # End-to-end tests only

# Code quality
make format            # Format code with black
make lint              # Lint code with flake8
make check             # Run linting and tests

# Clean build artifacts
make clean-build
```

See `make help` for all available commands.

### Basic Usage with AppRunner

```python
from acm_forecast.core import AppRunner

# Initialize with configuration file
runner = AppRunner(config_path="acm_forecast/examples/config_end_to_end.yaml")

# Run complete pipeline (load data, train, forecast, evaluate)
runner.run()

# Run specific steps
runner.run(steps=['load_data', 'prepare_data', 'train_model', 'forecast'])

# Generate forecasts only
runner.generate_forecasts(category="Total")
```

### Using Pipeline Classes Directly

```python
from pyspark.sql import SparkSession
from acm_forecast.config import AppConfig
from acm_forecast.pipeline import TrainingPipeline

# Load configuration
config = AppConfig.from_yaml("path/to/config.yaml")

# Initialize Spark
spark = SparkSession.builder.appName("ACM_Forecasting").getOrCreate()

# Create and run training pipeline
pipeline = TrainingPipeline(config, spark)
results = pipeline.run(category="Total")
```

### End-to-End Example

```bash
# Run complete end-to-end example
python -m acm_forecast.examples.run_end_to_end --config acm_forecast/examples/config_end_to_end.yaml
```

## ğŸ“– Documentation

- **[Installation Guide](INSTALLATION.md)** - Setup and installation instructions
- **[Configuration Guide](acm_forecast/config/README.md)** - YAML configuration reference
- **[Model Documentation](acm_forecast/MODEL_DOCUMENTATION.md)** - Comprehensive model documentation
- **[Examples Guide](acm_forecast/examples/README_E2E.md)** - End-to-end examples and usage
- **[Testing Guide](tests/README.md)** - Test suite documentation
- **[Packaging Guide](PACKAGING.md)** - Building and distributing the package

## ğŸ”Œ Plugin Architecture

The framework uses a plugin-based architecture for extensibility. All components (data sources, models, forecasters, etc.) are implemented as plugins.

### Built-in Plugins

- **Data Source**: `acm` - Azure Cost Management Delta data source
- **Data Quality**: `default` - Comprehensive data quality validation
- **Data Preparation**: `acm` - ACM-specific data preparation
- **Feature Engineer**: `default` - Temporal, lag, and rolling features
- **Models**: `prophet`, `arima`, `xgboost` - Forecasting models
- **Forecaster**: `default` - Forecast generation
- **Model Registry**: `mlflow` - MLflow model registry

### Plugin Configuration

Plugins are configured in YAML:

```yaml
plugins:
  data_source:
    name: "acm"
    config: {}
  data_quality:
    name: "default"
    config:
      additional_completeness_columns: ["meter_category", "resource_location"]
      currency_column: "billing_currency_code"
      expected_currency: "USD"
  data_preparation:
    name: "acm"
    config: {}
  feature_engineer:
    name: "default"
    config:
      quantity_column: "quantity"
  model:
    name: "prophet"  # Uses model.selected_model if not specified
    config: {}
  forecaster:
    name: "default"
    config: {}
  model_registry:
    name: "mlflow"
    config: {}
```

See [Configuration Guide](acm_forecast/config/README.md) for details.

## âš™ï¸ Configuration

Configuration is managed through YAML files with Pydantic validation. Example configuration:

```yaml
name: "acm_forecast"
data:
  delta_table_path: "azure_cost_management.amortized_costs"
  database_name: "azure_cost_management"
  table_name: "amortized_costs"
model:
  selected_model: "prophet"
  prophet:
    yearly_seasonality: true
    weekly_seasonality: true
training:
  train_split: 0.7
  validation_split: 0.15
  test_split: 0.15
forecast:
  forecast_horizons_days: [30, 60, 90]
```

See `acm_forecast/config/config.example.yaml` for the complete configuration template.

## ğŸ“Š Model Comparison

The framework provides comprehensive comparison metrics:

- **RMSE** (Root Mean Square Error)
- **MAE** (Mean Absolute Error)
- **MAPE** (Mean Absolute Percentage Error)
- **RÂ²** (Coefficient of Determination)

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run with coverage
make test-cov

# Run specific test types
pytest tests/unit -m unit
pytest tests/integration -m integration
pytest tests/e2e -m e2e
```

See [tests/README.md](tests/README.md) for detailed testing documentation.

## ğŸ“¦ Building and Distribution

```bash
# Build wheel package
make build

# Install locally
pip install dist/acm_forecast-1.0.0-py3-none-any.whl

# Install in development mode
pip install -e ".[dev]"
```

See [PACKAGING.md](PACKAGING.md) for detailed packaging instructions.

## ğŸ³ Docker Support

The project includes Docker configurations for development and production:

```bash
# Build Docker images
make docker-build

# Start services
make docker-up

# Run tests in Docker
make docker-test

# Run pipeline in Docker
make docker-run-pipeline
```

See `Makefile` help (`make help`) for all Docker commands.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test`
5. Run linting: `make lint`
6. Submit a pull request

## ğŸ“ License

MIT License

## ğŸ”— Related Resources

- [Prophet Documentation](https://facebook.github.io/prophet/)
- [MLflow Documentation](https://www.mlflow.org/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
