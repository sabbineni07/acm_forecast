{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Forecasting Model for Azure Cost Management\n",
    "\n",
    "This notebook implements XGBoost (Extreme Gradient Boosting) for Azure cost prediction. XGBoost is a powerful machine learning algorithm that can capture complex non-linear relationships and handle various data types effectively.\n",
    "\n",
    "## XGBoost Model Features\n",
    "- **Gradient Boosting**: Uses ensemble of decision trees with gradient descent optimization\n",
    "- **Feature Engineering**: Can incorporate multiple features including time-based, categorical, and numerical\n",
    "- **Non-linear Relationships**: Captures complex patterns that traditional time series methods might miss\n",
    "- **Robust Performance**: Handles missing values and outliers well\n",
    "- **Feature Importance**: Provides insights into which features drive cost predictions\n",
    "\n",
    "## Objectives\n",
    "1. Load and prepare time series data for XGBoost\n",
    "2. Create comprehensive feature engineering pipeline\n",
    "3. Train XGBoost models for different cost categories\n",
    "4. Implement time series cross-validation\n",
    "5. Generate forecasts and evaluate model performance\n",
    "6. Analyze feature importance and model interpretability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/sabbineni/projects/acm/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/sabbineni/projects/acm/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# XGBoost and ML specific imports\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesSplit\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, r2_score\n",
      "File \u001b[0;32m~/projects/acm/venv/lib/python3.9/site-packages/xgboost/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective, dask\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Booster,\n\u001b[1;32m     10\u001b[0m     DataIter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     build_info,\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/projects/acm/venv/lib/python3.9/site-packages/xgboost/tracker.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, make_jcargs\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/acm/venv/lib/python3.9/site-packages/xgboost/core.py:269\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m        return value from API calls\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/acm/venv/lib/python3.9/site-packages/xgboost/core.py:222\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[1;32m    221\u001b[0m         libname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(lib_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 222\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) could not be loaded.\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124mLikely causes:\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124m  * OpenMP runtime is not installed\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;124m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[38;5;124mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    237\u001b[0m     _register_log_callback(lib)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(ver: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[0;31mXGBoostError\u001b[0m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/sabbineni/projects/acm/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <89AD948E-E564-3266-867D-7AF89D6488F0> /Users/sabbineni/projects/acm/venv/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost and ML specific imports\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import shap\n",
    "\n",
    "# Load forecasting data\n",
    "import pickle\n",
    "with open('/Users/sabbineni/projects/acm/data/forecasting_data.pkl', 'rb') as f:\n",
    "    forecasting_data = pickle.load(f)\n",
    "\n",
    "# Load the main dataset for feature engineering\n",
    "df = pd.read_csv('/Users/sabbineni/projects/acm/data/sample_azure_costs.csv')\n",
    "df['UsageDateTime'] = pd.to_datetime(df['UsageDateTime'])\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Available time series: {list(forecasting_data.keys())}\")\n",
    "print(f\"Main dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for XGBoost\n",
    "def create_features(df, target_col='PreTaxCost'):\n",
    "    \"\"\"\n",
    "    Create comprehensive features for XGBoost model.\n",
    "    \"\"\"\n",
    "    print(\"Creating features for XGBoost model...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df_features['Year'] = df_features['UsageDateTime'].dt.year\n",
    "    df_features['Month'] = df_features['UsageDateTime'].dt.month\n",
    "    df_features['Day'] = df_features['UsageDateTime'].dt.day\n",
    "    df_features['DayOfWeek'] = df_features['UsageDateTime'].dt.dayofweek\n",
    "    df_features['DayOfYear'] = df_features['UsageDateTime'].dt.dayofyear\n",
    "    df_features['WeekOfYear'] = df_features['UsageDateTime'].dt.isocalendar().week\n",
    "    df_features['Quarter'] = df_features['UsageDateTime'].dt.quarter\n",
    "    df_features['IsWeekend'] = (df_features['DayOfWeek'] >= 5).astype(int)\n",
    "    df_features['IsMonthStart'] = df_features['UsageDateTime'].dt.is_month_start.astype(int)\n",
    "    df_features['IsMonthEnd'] = df_features['UsageDateTime'].dt.is_month_end.astype(int)\n",
    "    df_features['IsQuarterStart'] = df_features['UsageDateTime'].dt.is_quarter_start.astype(int)\n",
    "    df_features['IsQuarterEnd'] = df_features['UsageDateTime'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    # Cyclical encoding for time features\n",
    "    df_features['Month_sin'] = np.sin(2 * np.pi * df_features['Month'] / 12)\n",
    "    df_features['Month_cos'] = np.cos(2 * np.pi * df_features['Month'] / 12)\n",
    "    df_features['DayOfWeek_sin'] = np.sin(2 * np.pi * df_features['DayOfWeek'] / 7)\n",
    "    df_features['DayOfWeek_cos'] = np.cos(2 * np.pi * df_features['DayOfWeek'] / 7)\n",
    "    df_features['DayOfYear_sin'] = np.sin(2 * np.pi * df_features['DayOfYear'] / 365)\n",
    "    df_features['DayOfYear_cos'] = np.cos(2 * np.pi * df_features['DayOfYear'] / 365)\n",
    "    \n",
    "    # Lag features (past values)\n",
    "    df_features = df_features.sort_values('UsageDateTime')\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:\n",
    "        df_features[f'{target_col}_lag_{lag}'] = df_features.groupby('MeterCategory')[target_col].shift(lag)\n",
    "    \n",
    "    # Rolling window features\n",
    "    for window in [3, 7, 14, 30]:\n",
    "        df_features[f'{target_col}_rolling_mean_{window}'] = df_features.groupby('MeterCategory')[target_col].rolling(window=window).mean().reset_index(0, drop=True)\n",
    "        df_features[f'{target_col}_rolling_std_{window}'] = df_features.groupby('MeterCategory')[target_col].rolling(window=window).std().reset_index(0, drop=True)\n",
    "        df_features[f'{target_col}_rolling_max_{window}'] = df_features.groupby('MeterCategory')[target_col].rolling(window=window).max().reset_index(0, drop=True)\n",
    "        df_features[f'{target_col}_rolling_min_{window}'] = df_features.groupby('MeterCategory')[target_col].rolling(window=window).min().reset_index(0, drop=True)\n",
    "    \n",
    "    # Exponential moving averages\n",
    "    for span in [3, 7, 14]:\n",
    "        df_features[f'{target_col}_ema_{span}'] = df_features.groupby('MeterCategory')[target_col].ewm(span=span).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # Categorical features encoding\n",
    "    categorical_features = ['MeterCategory', 'MeterSubCategory', 'ResourceLocation', 'ServiceTier', 'Currency']\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in df_features.columns:\n",
    "            le = LabelEncoder()\n",
    "            df_features[f'{feature}_encoded'] = le.fit_transform(df_features[feature].astype(str))\n",
    "    \n",
    "    # Interaction features\n",
    "    df_features['UsageQuantity_x_ResourceRate'] = df_features['UsageQuantity'] * df_features['ResourceRate']\n",
    "    df_features['Cost_per_Unit'] = df_features['PreTaxCost'] / (df_features['UsageQuantity'] + 1e-8)\n",
    "    \n",
    "    # Statistical features by category\n",
    "    category_stats = df_features.groupby('MeterCategory')[target_col].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    category_stats.columns = ['MeterCategory', 'Category_mean', 'Category_std', 'Category_min', 'Category_max']\n",
    "    df_features = df_features.merge(category_stats, on='MeterCategory', how='left')\n",
    "    \n",
    "    # Relative features\n",
    "    df_features['Cost_vs_Category_mean'] = df_features[target_col] / (df_features['Category_mean'] + 1e-8)\n",
    "    df_features['Cost_vs_Category_std'] = (df_features[target_col] - df_features['Category_mean']) / (df_features['Category_std'] + 1e-8)\n",
    "    \n",
    "    print(f\"Created {len(df_features.columns)} features\")\n",
    "    return df_features\n",
    "\n",
    "# Create features\n",
    "df_features = create_features(df)\n",
    "print(f\"Feature engineering completed. Dataset shape: {df_features.shape}\")\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\nFeature categories:\")\n",
    "time_features = [col for col in df_features.columns if any(x in col for x in ['Year', 'Month', 'Day', 'Week', 'Quarter', 'sin', 'cos'])]\n",
    "lag_features = [col for col in df_features.columns if 'lag' in col]\n",
    "rolling_features = [col for col in df_features.columns if 'rolling' in col]\n",
    "categorical_features = [col for col in df_features.columns if 'encoded' in col]\n",
    "\n",
    "print(f\"Time features: {len(time_features)}\")\n",
    "print(f\"Lag features: {len(lag_features)}\")\n",
    "print(f\"Rolling features: {len(rolling_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Other features: {len(df_features.columns) - len(time_features) - len(lag_features) - len(rolling_features) - len(categorical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for XGBoost Training\n",
    "def prepare_xgboost_data(df_features, target_col='PreTaxCost', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for XGBoost training with proper train/test split for time series.\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for XGBoost training...\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_sorted = df_features.sort_values('UsageDateTime').reset_index(drop=True)\n",
    "    \n",
    "    # Select features (exclude target and non-predictive columns)\n",
    "    exclude_cols = [\n",
    "        'UsageDateTime', target_col, 'SubscriptionGuid', 'MeterId', 'InstanceId',\n",
    "        'Tags', 'OfferId', 'AdditionalInfo', 'ServiceInfo1', 'ServiceInfo2',\n",
    "        'MeterName', 'MeterRegion', 'ConsumedService', 'ResourceType',\n",
    "        'MeterCategory', 'MeterSubCategory', 'ResourceLocation', 'ServiceTier', 'Currency'\n",
    "    ]\n",
    "    \n",
    "    feature_cols = [col for col in df_sorted.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Remove columns with too many NaN values\n",
    "    nan_threshold = 0.5\n",
    "    valid_features = []\n",
    "    for col in feature_cols:\n",
    "        if df_sorted[col].isna().sum() / len(df_sorted) < nan_threshold:\n",
    "            valid_features.append(col)\n",
    "        else:\n",
    "            print(f\"Removing {col} due to high NaN ratio\")\n",
    "    \n",
    "    print(f\"Using {len(valid_features)} features for training\")\n",
    "    \n",
    "    # Create feature matrix and target\n",
    "    X = df_sorted[valid_features].fillna(0)  # Fill remaining NaN with 0\n",
    "    y = df_sorted[target_col]\n",
    "    \n",
    "    # Time series split (use last portion for testing)\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    \n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, valid_features\n",
    "\n",
    "# Prepare data for each category\n",
    "key_categories = ['Total', 'Compute', 'Storage', 'Database']\n",
    "xgboost_data = {}\n",
    "\n",
    "for category in key_categories:\n",
    "    if category == 'Total':\n",
    "        # Use all data for total cost prediction\n",
    "        X_train, X_test, y_train, y_test, features = prepare_xgboost_data(df_features)\n",
    "    else:\n",
    "        # Filter data for specific category\n",
    "        category_data = df_features[df_features['MeterCategory'] == category].copy()\n",
    "        if len(category_data) > 100:  # Need sufficient data\n",
    "            X_train, X_test, y_train, y_test, features = prepare_xgboost_data(category_data)\n",
    "        else:\n",
    "            print(f\"Skipping {category} - insufficient data\")\n",
    "            continue\n",
    "    \n",
    "    xgboost_data[category] = {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'features': features\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{category} data prepared:\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Test samples: {len(X_test)}\")\n",
    "    print(f\"  Features: {len(features)}\")\n",
    "\n",
    "print(f\"\\nData prepared for categories: {list(xgboost_data.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Models\n",
    "def train_xgboost_model(X_train, X_test, y_train, y_test, category_name):\n",
    "    \"\"\"\n",
    "    Train XGBoost model with hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining XGBoost model for {category_name}...\")\n",
    "    \n",
    "    # XGBoost parameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    \n",
    "    # Train model with early stopping\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(dtrain)\n",
    "    y_pred_test = model.predict(dtest)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"Training MAE: {train_mae:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Training RÂ²: {train_r2:.4f}\")\n",
    "    print(f\"Test RÂ²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = model.get_score(importance_type='weight')\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': list(importance.keys()),\n",
    "        'importance': list(importance.values())\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'metrics': {\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2\n",
    "        },\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "\n",
    "# Train models for all categories\n",
    "xgboost_models = {}\n",
    "\n",
    "for category, data in xgboost_data.items():\n",
    "    result = train_xgboost_model(\n",
    "        data['X_train'], data['X_test'],\n",
    "        data['y_train'], data['y_test'],\n",
    "        category\n",
    "    )\n",
    "    xgboost_models[category] = result\n",
    "\n",
    "print(f\"\\nSuccessfully trained XGBoost models for: {list(xgboost_models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "def plot_feature_importance(feature_importance, category_name, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance for XGBoost model.\n",
    "    \"\"\"\n",
    "    # Get top N features\n",
    "    top_features = feature_importance.head(top_n)\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=top_features['importance'],\n",
    "            y=top_features['feature'],\n",
    "            orientation='h',\n",
    "            marker_color='lightblue'\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Top {top_n} Feature Importance - {category_name}',\n",
    "        xaxis_title='Importance',\n",
    "        yaxis_title='Features',\n",
    "        height=600,\n",
    "        yaxis={'categoryorder': 'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Plot feature importance for each model\n",
    "for category, result in xgboost_models.items():\n",
    "    plot_feature_importance(result['feature_importance'], category)\n",
    "\n",
    "# Model Performance Comparison\n",
    "print(\"=== Model Performance Summary ===\")\n",
    "performance_summary = pd.DataFrame()\n",
    "\n",
    "for category, result in xgboost_models.items():\n",
    "    metrics = result['metrics']\n",
    "    performance_summary[category] = [\n",
    "        metrics['test_rmse'],\n",
    "        metrics['test_mae'],\n",
    "        metrics['test_r2']\n",
    "    ]\n",
    "\n",
    "performance_summary.index = ['RMSE', 'MAE', 'RÂ²']\n",
    "performance_summary = performance_summary.round(4)\n",
    "\n",
    "print(performance_summary)\n",
    "\n",
    "# Create performance visualization\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('RMSE Comparison', 'MAE Comparison', 'RÂ² Comparison'),\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    ")\n",
    "\n",
    "# RMSE\n",
    "fig.add_trace(\n",
    "    go.Bar(x=performance_summary.columns, y=performance_summary.loc['RMSE'],\n",
    "           name='RMSE', marker_color='red'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# MAE\n",
    "fig.add_trace(\n",
    "    go.Bar(x=performance_summary.columns, y=performance_summary.loc['MAE'],\n",
    "           name='MAE', marker_color='blue'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# RÂ²\n",
    "fig.add_trace(\n",
    "    go.Bar(x=performance_summary.columns, y=performance_summary.loc['RÂ²'],\n",
    "           name='RÂ²', marker_color='green'),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"XGBoost Model Performance Comparison\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Future Forecasts\n",
    "def generate_future_forecasts(model, last_data, features, periods=30):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using trained XGBoost model.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {periods}-day forecasts...\")\n",
    "    \n",
    "    # Create future dates\n",
    "    last_date = last_data['UsageDateTime'].iloc[-1]\n",
    "    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=periods, freq='D')\n",
    "    \n",
    "    # Create future features (simplified approach)\n",
    "    future_data = []\n",
    "    \n",
    "    for i, date in enumerate(future_dates):\n",
    "        # Create basic time features\n",
    "        future_row = {\n",
    "            'Year': date.year,\n",
    "            'Month': date.month,\n",
    "            'Day': date.day,\n",
    "            'DayOfWeek': date.dayofweek,\n",
    "            'DayOfYear': date.dayofyear,\n",
    "            'WeekOfYear': date.isocalendar().week,\n",
    "            'Quarter': date.quarter,\n",
    "            'IsWeekend': 1 if date.dayofweek >= 5 else 0,\n",
    "            'IsMonthStart': 1 if date.day == 1 else 0,\n",
    "            'IsMonthEnd': 1 if date == (date + pd.offsets.MonthEnd(0)) else 0,\n",
    "            'IsQuarterStart': 1 if date in [pd.Timestamp(f'{date.year}-01-01'), \n",
    "                                          pd.Timestamp(f'{date.year}-04-01'),\n",
    "                                          pd.Timestamp(f'{date.year}-07-01'),\n",
    "                                          pd.Timestamp(f'{date.year}-10-01')] else 0,\n",
    "            'IsQuarterEnd': 1 if date in [pd.Timestamp(f'{date.year}-03-31'),\n",
    "                                        pd.Timestamp(f'{date.year}-06-30'),\n",
    "                                        pd.Timestamp(f'{date.year}-09-30'),\n",
    "                                        pd.Timestamp(f'{date.year}-12-31')] else 0\n",
    "        }\n",
    "        \n",
    "        # Add cyclical features\n",
    "        future_row['Month_sin'] = np.sin(2 * np.pi * date.month / 12)\n",
    "        future_row['Month_cos'] = np.cos(2 * np.pi * date.month / 12)\n",
    "        future_row['DayOfWeek_sin'] = np.sin(2 * np.pi * date.dayofweek / 7)\n",
    "        future_row['DayOfWeek_cos'] = np.cos(2 * np.pi * date.dayofweek / 7)\n",
    "        future_row['DayOfYear_sin'] = np.sin(2 * np.pi * date.dayofyear / 365)\n",
    "        future_row['DayOfYear_cos'] = np.cos(2 * np.pi * date.dayofyear / 365)\n",
    "        \n",
    "        # Add lag features (use recent averages)\n",
    "        recent_avg = last_data['PreTaxCost'].tail(7).mean()\n",
    "        for lag in [1, 2, 3, 7, 14, 30]:\n",
    "            future_row[f'PreTaxCost_lag_{lag}'] = recent_avg\n",
    "        \n",
    "        # Add rolling features\n",
    "        for window in [3, 7, 14, 30]:\n",
    "            future_row[f'PreTaxCost_rolling_mean_{window}'] = recent_avg\n",
    "            future_row[f'PreTaxCost_rolling_std_{window}'] = last_data['PreTaxCost'].tail(7).std()\n",
    "            future_row[f'PreTaxCost_rolling_max_{window}'] = last_data['PreTaxCost'].tail(7).max()\n",
    "            future_row[f'PreTaxCost_rolling_min_{window}'] = last_data['PreTaxCost'].tail(7).min()\n",
    "        \n",
    "        # Add EMA features\n",
    "        for span in [3, 7, 14]:\n",
    "            future_row[f'PreTaxCost_ema_{span}'] = recent_avg\n",
    "        \n",
    "        # Add other features with default values\n",
    "        for feature in features:\n",
    "            if feature not in future_row:\n",
    "                future_row[feature] = 0\n",
    "        \n",
    "        future_data.append(future_row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    future_df = pd.DataFrame(future_data)\n",
    "    \n",
    "    # Ensure all features are present\n",
    "    for feature in features:\n",
    "        if feature not in future_df.columns:\n",
    "            future_df[feature] = 0\n",
    "    \n",
    "    # Reorder columns to match training data\n",
    "    future_df = future_df[features]\n",
    "    \n",
    "    # Make predictions\n",
    "    dmatrix = xgb.DMatrix(future_df)\n",
    "    forecasts = model.predict(dmatrix)\n",
    "    \n",
    "    return future_dates, forecasts\n",
    "\n",
    "# Generate forecasts for each model\n",
    "future_forecasts = {}\n",
    "\n",
    "for category, result in xgboost_models.items():\n",
    "    print(f\"\\nGenerating forecasts for {category}...\")\n",
    "    \n",
    "    # Get the last data for this category\n",
    "    if category == 'Total':\n",
    "        last_data = df_features.tail(100)  # Use last 100 records\n",
    "    else:\n",
    "        last_data = df_features[df_features['MeterCategory'] == category].tail(100)\n",
    "    \n",
    "    future_dates, forecasts = generate_future_forecasts(\n",
    "        result['model'], \n",
    "        last_data, \n",
    "        xgboost_data[category]['features']\n",
    "    )\n",
    "    \n",
    "    future_forecasts[category] = {\n",
    "        'dates': future_dates,\n",
    "        'forecasts': forecasts\n",
    "    }\n",
    "    \n",
    "    print(f\"Forecast period: {future_dates[0]} to {future_dates[-1]}\")\n",
    "    print(f\"Average predicted cost: ${forecasts.mean():.2f}\")\n",
    "    print(f\"Total predicted cost: ${forecasts.sum():.2f}\")\n",
    "\n",
    "print(f\"\\nFuture forecasts generated for: {list(future_forecasts.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost Results\n",
    "print(\"=== Saving XGBoost Results ===\")\n",
    "\n",
    "# Save models and results\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create results directory\n",
    "results_dir = '/Users/sabbineni/projects/acm/results/xgboost'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "for category, result in xgboost_models.items():\n",
    "    model_path = f\"{results_dir}/xgboost_model_{category.lower()}.pkl\"\n",
    "    joblib.dump(result['model'], model_path)\n",
    "    print(f\"Saved model: {model_path}\")\n",
    "\n",
    "# Save performance metrics\n",
    "performance_data = {}\n",
    "for category, result in xgboost_models.items():\n",
    "    performance_data[category] = result['metrics']\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data).T\n",
    "performance_path = f\"{results_dir}/xgboost_performance.csv\"\n",
    "performance_df.to_csv(performance_path)\n",
    "print(f\"Saved performance metrics: {performance_path}\")\n",
    "\n",
    "# Save feature importance\n",
    "for category, result in xgboost_models.items():\n",
    "    importance_path = f\"{results_dir}/xgboost_feature_importance_{category.lower()}.csv\"\n",
    "    result['feature_importance'].to_csv(importance_path, index=False)\n",
    "    print(f\"Saved feature importance: {importance_path}\")\n",
    "\n",
    "# Save future forecasts\n",
    "for category, forecast_data in future_forecasts.items():\n",
    "    forecast_df = pd.DataFrame({\n",
    "        'date': forecast_data['dates'],\n",
    "        'forecast': forecast_data['forecasts']\n",
    "    })\n",
    "    \n",
    "    forecast_path = f\"{results_dir}/xgboost_forecast_{category.lower()}.csv\"\n",
    "    forecast_df.to_csv(forecast_path, index=False)\n",
    "    print(f\"Saved forecast: {forecast_path}\")\n",
    "\n",
    "# Create forecast comparison\n",
    "forecast_comparison = pd.DataFrame()\n",
    "for category, forecast_data in future_forecasts.items():\n",
    "    forecast_comparison[category] = forecast_data['forecasts']\n",
    "\n",
    "forecast_comparison.index = future_forecasts['Total']['dates']\n",
    "forecast_comparison.index.name = 'Date'\n",
    "\n",
    "comparison_path = f\"{results_dir}/xgboost_forecast_comparison.csv\"\n",
    "forecast_comparison.to_csv(comparison_path)\n",
    "print(f\"Saved forecast comparison: {comparison_path}\")\n",
    "\n",
    "# Save feature engineering data\n",
    "features_path = f\"{results_dir}/feature_engineering_data.csv\"\n",
    "df_features.to_csv(features_path, index=False)\n",
    "print(f\"Saved feature engineering data: {features_path}\")\n",
    "\n",
    "print(\"\\nâœ… XGBoost model implementation completed successfully!\")\n",
    "print(\"ðŸ“Š Models trained, evaluated, and saved\")\n",
    "print(\"ðŸ”® Future forecasts generated for 30 days\")\n",
    "print(\"ðŸ“ˆ Feature importance analysis completed\")\n",
    "print(\"ðŸŽ¯ Results ready for comparison with other models\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
